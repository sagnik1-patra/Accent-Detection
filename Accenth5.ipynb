{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed355fbf-a10a-429d-b37a-8a9f9708e7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keeping 35 classes: {'russian', 'bengali', 'italian', 'hindi', 'spanish', 'nepali', 'amharic', 'swedish', 'thai', 'tagalog', 'japanese', 'vietnamese', 'german', 'punjabi', 'urdu', 'cantonese', 'romanian', 'french', 'farsi', 'bulgarian', 'ukrainian', 'korean', 'dutch', 'pashto', 'macedonian', 'greek', 'polish', 'mandarin', 'serbian', 'arabic', 'english', 'turkish', 'kurdish', 'portuguese', 'miskito'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [04:07<00:00,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature shape: (1144, 33, 130), Labels: (1144,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_dir = r\"C:\\Users\\sagni\\Downloads\\Accent Detectection\\cleaned_dataset\"\n",
    "MIN_SAMPLES = 10\n",
    "MAX_FILES_PER_CLASS = 100  # Optional limit\n",
    "\n",
    "def extract_features(file_path, sr=22050, max_len=130):\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    # Extract MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Extract Chroma\n",
    "    stft = np.abs(librosa.stft(y))\n",
    "    chroma = librosa.feature.chroma_stft(S=stft, sr=sr)\n",
    "\n",
    "    # Spectral Contrast\n",
    "    contrast = librosa.feature.spectral_contrast(S=stft, sr=sr)\n",
    "\n",
    "    # Zero Crossing Rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "\n",
    "    # Stack features into one matrix\n",
    "    features = np.vstack([mfcc, chroma, contrast, zcr])\n",
    "\n",
    "    # Pad/truncate to fixed time axis (second axis)\n",
    "    if features.shape[1] < max_len:\n",
    "        pad_width = max_len - features.shape[1]\n",
    "        features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        features = features[:, :max_len]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Count label frequencies\n",
    "label_counts = Counter()\n",
    "for label in os.listdir(base_dir):\n",
    "    path = os.path.join(base_dir, label)\n",
    "    if os.path.isdir(path):\n",
    "        label_counts[label] = len([f for f in os.listdir(path) if f.endswith('.mp3')])\n",
    "\n",
    "# Keep only classes with enough samples\n",
    "valid_labels = {label for label, count in label_counts.items() if count >= MIN_SAMPLES}\n",
    "print(f\"✅ Keeping {len(valid_labels)} classes: {valid_labels}\")\n",
    "\n",
    "# Extract features\n",
    "X = []\n",
    "y = []\n",
    "label_to_index = {label: idx for idx, label in enumerate(sorted(valid_labels))}\n",
    "\n",
    "for label in tqdm(valid_labels, desc=\"Extracting features\"):\n",
    "    label_path = os.path.join(base_dir, label)\n",
    "    files = [f for f in os.listdir(label_path) if f.endswith('.mp3')]\n",
    "    files = files[:MAX_FILES_PER_CLASS]  # Optional: limit per class\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            file_path = os.path.join(label_path, file)\n",
    "            features = extract_features(file_path)\n",
    "            X.append(features)\n",
    "            y.append(label_to_index[label])\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path}: {e}\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"✅ Feature shape: {X.shape}, Labels: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba8f2e5a-a6fa-4450-ac53-a9c5f7b13ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.0411 - loss: 17.2314 - val_accuracy: 0.0830 - val_loss: 3.5442\n",
      "Epoch 2/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.0606 - loss: 3.5476 - val_accuracy: 0.1004 - val_loss: 3.5187\n",
      "Epoch 3/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.0679 - loss: 3.5194 - val_accuracy: 0.1092 - val_loss: 3.5159\n",
      "Epoch 4/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.0644 - loss: 3.4713 - val_accuracy: 0.0961 - val_loss: 3.4888\n",
      "Epoch 5/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.1183 - loss: 3.3539 - val_accuracy: 0.1004 - val_loss: 3.4768\n",
      "Epoch 6/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.1021 - loss: 3.2986 - val_accuracy: 0.0873 - val_loss: 3.4660\n",
      "Epoch 7/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.1392 - loss: 3.2703 - val_accuracy: 0.1048 - val_loss: 3.4362\n",
      "Epoch 8/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.1359 - loss: 3.2745 - val_accuracy: 0.0742 - val_loss: 3.4505\n",
      "Epoch 9/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.1299 - loss: 3.1881 - val_accuracy: 0.1048 - val_loss: 3.4354\n",
      "Epoch 10/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.1463 - loss: 3.1807 - val_accuracy: 0.1179 - val_loss: 3.3954\n",
      "Epoch 11/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.1491 - loss: 3.1469 - val_accuracy: 0.1179 - val_loss: 3.4257\n",
      "Epoch 12/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.1450 - loss: 3.1349 - val_accuracy: 0.1135 - val_loss: 3.3958\n",
      "Epoch 13/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.1625 - loss: 3.0417 - val_accuracy: 0.1179 - val_loss: 3.3862\n",
      "Epoch 14/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.1729 - loss: 3.0423 - val_accuracy: 0.1092 - val_loss: 3.3924\n",
      "Epoch 15/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.1861 - loss: 2.9416 - val_accuracy: 0.1223 - val_loss: 3.3659\n",
      "Epoch 16/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.1735 - loss: 2.9053 - val_accuracy: 0.1179 - val_loss: 3.3659\n",
      "Epoch 17/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1895 - loss: 2.8575 - val_accuracy: 0.1354 - val_loss: 3.3918\n",
      "Epoch 18/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1961 - loss: 2.8454 - val_accuracy: 0.1441 - val_loss: 3.3734\n",
      "Epoch 19/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.1932 - loss: 2.7650 - val_accuracy: 0.1266 - val_loss: 3.3929\n",
      "Epoch 20/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.2325 - loss: 2.7052 - val_accuracy: 0.1223 - val_loss: 3.4238\n",
      "Epoch 21/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.2186 - loss: 2.5850 - val_accuracy: 0.1266 - val_loss: 3.4677\n",
      "Epoch 22/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.2837 - loss: 2.5671 - val_accuracy: 0.1397 - val_loss: 3.4190\n",
      "Epoch 23/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.3020 - loss: 2.4375 - val_accuracy: 0.1397 - val_loss: 3.5208\n",
      "Epoch 24/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.2848 - loss: 2.4568 - val_accuracy: 0.1354 - val_loss: 3.5507\n",
      "Epoch 25/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3070 - loss: 2.3211 - val_accuracy: 0.1179 - val_loss: 3.5149\n",
      "Epoch 26/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.3282 - loss: 2.2564 - val_accuracy: 0.1135 - val_loss: 3.6437\n",
      "Epoch 27/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3842 - loss: 2.0963 - val_accuracy: 0.0917 - val_loss: 3.6838\n",
      "Epoch 28/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.3843 - loss: 2.1063 - val_accuracy: 0.1092 - val_loss: 3.6300\n",
      "Epoch 29/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.4205 - loss: 1.9704 - val_accuracy: 0.1092 - val_loss: 3.7198\n",
      "Epoch 30/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.4107 - loss: 1.9484 - val_accuracy: 0.1004 - val_loss: 3.8177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Prepare input shape\n",
    "X = X[..., np.newaxis]  # (samples, features, time, 1)\n",
    "y_cat = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=X.shape[1:]),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28901e6-4069-49df-b7a9-beb1ed1edf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
